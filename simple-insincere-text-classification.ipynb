{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Notebook Objective**: \n\nSimple approach to build a sincere/insincere classifier for Quora questions.\n\n- **Data preprocssing**\n   \n   Data Exploration: few statistics about the questions text.\n   \n   Building the vocabulary and then tokenizing the questions text.\n    \n\n\n- **Modelisation**\n    \n    Straight-forward Many-to-One approach with a LSTM layer. LSTM are particular RNN that prevent vanishing gradient and helps the model to better 'remember' when dealing with long sequences. For a better understanding of what is going on under the hood see: [Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n    \n    We fed the model with embedded words. In this notebook we will use our own embeddings (trained on the top of the model) rather than the pre-trained ones given as auxiliary inputs. Despite an additional training step for our model, the idea here is that our embedding will be more dedicated to the specific task we are doing than a pre-trained embedding coming from Wikipedia pages.\n    \n    \n- **Predictions and submission**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport spacy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.preprocessing import sequence\nfrom keras.layers import Dense, Embedding, Dropout, LSTM\nfrom keras import Model\nfrom keras.optimizers import Adam  \nimport keras.backend as K\nfrom keras.callbacks import Callback\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration and Processing","metadata":{}},{"cell_type":"markdown","source":"In this section we will explore and make some preprocessing on the data to feed our model.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \", train.shape)\nprint(\"Test shape : \", test.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at the classes distribution.\ntrain.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have unbalanced classes: 6.2% insincere vs 93.8% sincere questions. Hence, the performance metrics can't be the accuracy, we will look at the F1-score (as mentioned in the challenge description).","metadata":{}},{"cell_type":"markdown","source":"## Some statistics","metadata":{}},{"cell_type":"code","source":"train['lenght_sentence'] = train['question_text'].apply(lambda x: len(x.split()))\nprint('Min questions lenght:', np.min(train['lenght_sentence'] ))\nprint('Max questions lenght:', np.max(train['lenght_sentence'] ))\nprint('Mean questions lenght:', np.mean(train['lenght_sentence'] ))\nprint('Standard deviation questions lenght:', np.std(train['lenght_sentence'] ))\n\n\n# Plot the distribution of the lenght of the questions\nplt.hist(train['lenght_sentence'], 100);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['lenght_sentence'] = test['question_text'].apply(lambda x: len(x.split()))\nprint('Min questions lenght:', np.min(test['lenght_sentence'] ))\nprint('Max questions lenght:', np.max(test['lenght_sentence'] ))\nprint('Mean questions lenght:', np.mean(test['lenght_sentence'] ))\nprint('Standard deviation questions lenght:', np.std(test['lenght_sentence'] ))\n\n# Plot the distribution of the lenght of the questions\nplt.hist(test['lenght_sentence'], 100);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the sentences are not of the same size, and we need them to be all at the same format to feed them to our model. \n\nTo cope with this issue we will truncate too long sentences and use a 0 padding for the short sentences. Thanks to the statistics we have extracted we will use a max_sentence_length = 20 (mean+stddev).","metadata":{}},{"cell_type":"markdown","source":"## Building the vocabulary","metadata":{}},{"cell_type":"markdown","source":"We are going to build a vocabulary of all the unique words in the Train and Test sets. ","metadata":{}},{"cell_type":"code","source":"# First let's lower all the words in our train and test sets\ntrain['question_text_truncated'] = train['question_text'].apply(lambda x: \" \".join([word.lower() for word in x.split()[:20]]))\ntest['question_text_truncated'] = test['question_text'].apply(lambda x: \" \".join([word.lower() for word in x.split()[:20]]))\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add all the questions\nlist_questions = list(train['question_text_truncated']) + list(test['question_text_truncated'])\n\n# Split the questions into words then join them all together and finally we remove duplicates\nunique_words = set((\" \".join(list_questions)).split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Give an index to each word staring from 2.\nindex_from = 2\n\n# Making the vocabulary\nvocabulary = {k: (v + index_from) for v, k in enumerate(unique_words)}\n\nvocabulary[\"<PAD>\"] = 0\nvocabulary[\"<START>\"] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Vocabulary length:', len(vocabulary))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization of the questions","metadata":{}},{"cell_type":"markdown","source":"Now that we have our vocabulary, we will tokenize our sentences and pad with 0 the smaller ones to reach a 134 sequence size.","metadata":{}},{"cell_type":"code","source":"# Tokenization of all words in a sentence using our vocabulary\ndef sentence_tokenization(sentence, vocabulary):\n    tokenized_sentence = []\n    for word in sentence.split():\n        tokenized_sentence.append(vocabulary[word])\n    return  tokenized_sentence\n    \n\ntrain[\"question_tokenized\"] = train[\"question_text_truncated\"].apply(lambda x: sentence_tokenization(x, vocabulary))\ntest[\"question_tokenized\"] = test[\"question_text_truncated\"].apply(lambda x: sentence_tokenization(x, vocabulary))\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_max_length = 20\n# 0 padding of the tokenized questions\nX = sequence.pad_sequences(train['question_tokenized'], maxlen = input_max_length, padding = \"post\", truncating= \"post\", value = 0)\nX_test = sequence.pad_sequences(test['question_tokenized'], maxlen = input_max_length, padding = \"post\", truncating= \"post\", value = 0)\n\ny = train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We prepare our data for the training and validation steps which we will make to avoid overfitting\n# Train/Validate split is less time consuming than several folds cross-validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelisation: Many-to-One model with LSTM layer and Embedding on top","metadata":{}},{"cell_type":"code","source":"embedding_vector_length = 150\ntotal_words = len(vocabulary) \ninputs_max_length = 20\n\nmodel = Sequential()\nmodel.add(Embedding(total_words, embedding_vector_length, input_length = inputs_max_length))\nmodel.add(LSTM(units = 256))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see above that the Embedding task gathers about 99% of our total number of parameters!","metadata":{}},{"cell_type":"code","source":"# We must build a custom F1 metrics to plug it into our training steps with Keras\ndef f1(y_true, y_pred):\n    '''\n    metric from here \n    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n    '''\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile and fit the model on our Train/Validate datasets\n\n#model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=[f1])\n#model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training on the whole dataset\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=[f1])\nmodel.fit(X, y, epochs=3, batch_size=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions and submission","metadata":{}},{"cell_type":"code","source":"pred_test = np.where(model.predict(X_test, batch_size=1024) < 0.5, 0, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npredictions = pd.DataFrame({\"qid\":test[\"qid\"].values})\npredictions['prediction'] = pred_test\npredictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.to_csv('submission.csv', index=False, sep=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}